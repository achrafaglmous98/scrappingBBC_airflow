from airflow import DAG
from airflow.contrib.sensors.file_sensor import FileSensor
from airflow.operators.python_operator import PythonOperator
import pandas as pd
from datetime import datetime, timedelta
import pymongo
import shutil
import os

def clean_and_push_data():
    # Find the most recent CSV file in the directory with the given prefix
    prefix = 'bbc_articles_'
    files = [f for f in os.listdir('./scrapped_data') if f.startswith(prefix) and f.endswith('.csv')]
    filename = min(files, key=os.path.getctime)
    filepath = f'./scrapped_data/{filename}'
    
    # Read data from CSV file
    df = pd.read_csv(filepath)
    
    # Clean data
    df = df[df['age'] > 30]
    
    # Push data to MongoDB
    client = pymongo.MongoClient('mongodb://localhost:27017/')
    db = client['mydatabase']
    collection = db['mycollection']
    collection.insert_many(df.to_dict('records'))
    
    # Move file to another directory
    dest_dir = './processed_data/'
    if not os.path.exists(dest_dir):
        os.makedirs(dest_dir)
    shutil.move(filepath, dest_dir)

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': datetime(2023, 3, 11),
    'retries': 1,
}

dag = DAG('clean_and_push_data', default_args=default_args, schedule_interval=None)

file_sensor_task = FileSensor(
    task_id='file_sensor_task',
    filepath='./scrapped_data/bbc_articles_*.csv',
    poke_interval=timedelta(hours=3),
    dag=dag,
)

clean_and_push_task = PythonOperator(
    task_id='clean_and_push_task',
    python_callable=clean_and_push_data,
    dag=dag,
)

file_sensor_task >> clean_and_push_task

from datetime import datetime
from pymongo import MongoClient
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import BranchPythonOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.email_operator import EmailOperator
from airflow.contrib.sensors.file_sensor import FileSensor

def clean_and_push_to_mongodb(**context):
    file_path = context['ti'].xcom_pull(key='file_path')
    # Clean the data
    cleaned_data = clean_data(file_path)
    # Push the cleaned data to MongoDB
    client = MongoClient('mongodb://localhost:27017/')
    db = client['my_db']
    collection = db['my_collection']
    collection.insert_many(cleaned_data)

dag = DAG(
    'process_bbc_data',
    description='Process BBC Data',
    schedule_interval=None,
    start_date=datetime(2023, 3, 11),
    catchup=False
)

wait_for_csv = FileSensor(
    task_id='wait_for_csv',
    filepath='/path/to/csv/file',
    poke_interval=10,
    dag=dag
)

clean_data_task = PythonOperator(
    task_id='clean_data_task',
    python_callable=clean_and_push_to_mongodb,
    provide_context=True,
    dag=dag
)

send_email = EmailOperator(
    task_id='send_email',
    to='youremail@example.com',
    subject='BBC Data Processing Completed',
    html_content='BBC Data Processing Completed Successfully',
    dag=dag
)

wait_for_csv >> clean_data_task >> send_email